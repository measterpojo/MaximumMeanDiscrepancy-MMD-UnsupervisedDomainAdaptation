{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOnzSbvkvWsU/7xFNW7r2NT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Introduction**"],"metadata":{"id":"9CeNfvYKgwzt"}},{"cell_type":"markdown","source":["**Maximum Mean Discrepancy**"],"metadata":{"id":"kceYhicagzOd"}},{"cell_type":"markdown","source":["Maximum Mean Discrepancy (MMD) is a kernel based statistical test used to determine whether two distributions are the same. It does this by measuring the distance between the means of the two distributions"],"metadata":{"id":"fDzOKm26jyeW"}},{"cell_type":"markdown","source":["MMD(X, Y) = ‚à•E[œÜ(X)] - E[œÜ(Y)]‚à•¬≤"],"metadata":{"id":"5UFDBci_kw7e"}},{"cell_type":"markdown","source":["ùê∏\n","[\n","ùúë\n","(\n","ùëã\n",")\n","]\n",": This is the expectation (mean) of the feature map\n","ùúë\n"," applied to the samples from distribution\n","ùëã\n",".\n","\n","ùê∏\n","[\n","ùúë\n","(\n","ùëå\n",")\n","]\n",": This is the expectation (mean) of the feature map\n","ùúë\n"," applied to the samples from distribution\n","ùëå\n",".\n","\n","‚à•\n",".\n",".\n",".\n","‚à• ^ 2\n",": This represents the squared norm (distance) between the two mean feature vectors."],"metadata":{"id":"yAFjUXHj6n9W"}},{"cell_type":"markdown","source":["Why use MMD compared to other loss function for DA??"],"metadata":{"id":"TrpnUXXmGeJX"}},{"cell_type":"markdown","source":["When it comes to domain adaptation (DA), MMD (Maximum Mean Discrepancy) has distinct advantages over Cross-Entropy Loss and Mean Squared Error (MSE) Loss due to its focus on aligning distributions rather than individual predictions. It directly addresses the issue of domain shift by aligning the distributions."],"metadata":{"id":"bXJV0optIB2u"}},{"cell_type":"markdown","source":["**Implementation of MMD**"],"metadata":{"id":"sr4X0GOdgzYM"}},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"-EDaZOa-840V"}},{"cell_type":"code","source":["import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torchvision\n","from torchvision import transforms\n","import torchvision.models as models\n","\n","from PIL import Image\n","\n","import xml.etree.ElementTree as ET"],"metadata":{"id":"gXwG9b1y851L","executionInfo":{"status":"ok","timestamp":1739823593910,"user_tz":360,"elapsed":2586,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Setup device-agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"7FlYB5G6UtT6","executionInfo":{"status":"ok","timestamp":1739823593911,"user_tz":360,"elapsed":14,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"9676e5d1-27a2-449c-bdec-c8ebe19329eb"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","\n","# Set device assertion flag for more detailed errors\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"2LLugOXgdeYy","executionInfo":{"status":"ok","timestamp":1739823593911,"user_tz":360,"elapsed":13,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Custom loss function\n","\n","- In the context of Maximum Mean Discrepancy (MMD), the loss function used is typically the MMD value itself. The goal is to minimize the MMD between two distributions, which effectively measures how similar they are in the feature space."],"metadata":{"id":"ADpsThHO_KNn"}},{"cell_type":"code","source":["class MMDLoss(nn.Module):\n","    def __init__(self, kernel_type='rbf'):\n","        super(MMDLoss, self).__init__()\n","        self.kernel_type = kernel_type\n","\n","    def gaussian_kernel(self, x, y, sigma=1.0):\n","        # Compute the Gaussian (RBF) kernel between x and y\n","        dist = torch.cdist(x, y)\n","        K = torch.exp(-dist ** 2 / (2 * sigma ** 2))\n","        return K\n","\n","    def forward(self, x, y):\n","        if self.kernel_type == 'rbf':\n","            return torch.mean(self.gaussian_kernel(x, x)) + \\\n","                   torch.mean(self.gaussian_kernel(y, y)) - \\\n","                   2 * torch.mean(self.gaussian_kernel(x, y))\n","        else:\n","            raise ValueError('Unknown kernel type: {}'.format(self.kernel_type))"],"metadata":{"id":"DfLezXzI_Y7B","executionInfo":{"status":"ok","timestamp":1739823593911,"user_tz":360,"elapsed":13,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Datasets"],"metadata":{"id":"d2xQsjrcgBvX"}},{"cell_type":"markdown","source":["\n","\n","*   Target data - CIFAR10\n","*   Source data - Pascal-voc-2012\n","\n"],"metadata":{"id":"pR4CYe6dDmGm"}},{"cell_type":"code","source":["# Define transformations for the training and test datasets\n","simple_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize images to a common size\n","    transforms.ToTensor(),  # Convert images to tensors\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"],"metadata":{"id":"yUVpuTE7BsBl","executionInfo":{"status":"ok","timestamp":1739823593911,"user_tz":360,"elapsed":12,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","def walk_through_dir(dir_path):\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"],"metadata":{"id":"cYokiq8Ag9en","executionInfo":{"status":"ok","timestamp":1739823593911,"user_tz":360,"elapsed":12,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Downloading the target dataset from torchvision datasets\n","target_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=simple_transform)\n","target_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=simple_transform)\n","\n","\n","target_train_dataloader = torch.utils.data.DataLoader(target_trainset, shuffle=True, batch_size=32)\n","target_test_dataloader = torch.utils.data.DataLoader(target_testset, shuffle=True,batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fde_mQ0CBQ35","executionInfo":{"status":"ok","timestamp":1739823595177,"user_tz":360,"elapsed":1278,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"4f738ba9-0048-49c9-db59-391356a1f338"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# Downlaoding the Source dataset from kaggle\n","import kagglehub\n","\n","source_path = kagglehub.dataset_download(\"gopalbhattrai/pascal-voc-2012-dataset\")\n","\n","print(\"Path to source_dataset files:\", source_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gy4m9gIloyP2","executionInfo":{"status":"ok","timestamp":1739823597763,"user_tz":360,"elapsed":2588,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"de905502-6fee-4d5d-f58c-6880df69bfb8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n","Path to source_dataset files: /root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1\n"]}]},{"cell_type":"code","source":["walk_through_dir(source_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"7QxkSQ0B2wul","executionInfo":{"status":"ok","timestamp":1739823597763,"user_tz":360,"elapsed":10,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"da769160-516c-480f-e36f-54b37edc5d78"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 2 directories and 0 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1'.\n","There are 1 directories and 0 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val'.\n","There are 5 directories and 1 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val'.\n","There are 0 directories and 2913 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/SegmentationObject'.\n","There are 0 directories and 17125 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/JPEGImages'.\n","There are 0 directories and 17125 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/Annotations'.\n","There are 4 directories and 0 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/ImageSets'.\n","There are 0 directories and 3 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/ImageSets/Layout'.\n","There are 0 directories and 3 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/ImageSets/Segmentation'.\n","There are 0 directories and 33 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/ImageSets/Action'.\n","There are 0 directories and 63 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/ImageSets/Main'.\n","There are 0 directories and 2913 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/SegmentationClass'.\n","There are 1 directories and 0 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test'.\n","There are 3 directories and 0 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test'.\n","There are 0 directories and 16135 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/JPEGImages'.\n","There are 0 directories and 5138 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/Annotations'.\n","There are 4 directories and 0 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/ImageSets'.\n","There are 0 directories and 1 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/ImageSets/Layout'.\n","There are 0 directories and 1 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/ImageSets/Segmentation'.\n","There are 0 directories and 11 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/ImageSets/Action'.\n","There are 0 directories and 21 images in '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_test/VOC2012_test/ImageSets/Main'.\n"]}]},{"cell_type":"code","source":["from __future__ import annotations\n","# Custom dataset for the Pascal-2012\n","class PascalDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","        self.image_list = os.listdir(self.image_dir)\n","\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        image_name = self.image_list[idx]\n","        image_path = os.path.join(self.image_dir, image_name)\n","        # annotation_path = os.path.join(self.annotation_dir).\n","\n","        image = Image.open(image_path).convert('RGB')\n","\n","        annotation_path = os.path.join(self.annotation_dir, image_name.replace('.jpg', '.xml'))\n","\n","        # Parse the annotation file\n","        tree = ET.parse(annotation_path)\n","        root = tree.getroot()\n","        label = root.find('object').find('name').text\n","\n","        # Convert label to an integer (assuming labels are stored as strings and are convertible to integers)\n","        label_dict = {\n","            'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n","            'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n","            'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n","            'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19\n","        }\n","        label = label_dict[label]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, torch.tensor(label)"],"metadata":{"id":"IJBuiUzkAnh9","executionInfo":{"status":"ok","timestamp":1739823597763,"user_tz":360,"elapsed":9,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["image_dir = '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/JPEGImages'\n","annotation_dir = '/root/.cache/kagglehub/datasets/gopalbhattrai/pascal-voc-2012-dataset/versions/1/VOC2012_train_val/VOC2012_train_val/Annotations'\n","\n","pascal_voc_dataset = PascalDataset(image_dir=image_dir, annotation_dir=annotation_dir, transform=simple_transform)\n","\n","source_loader = torch.utils.data.DataLoader(pascal_voc_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"ymzuZDB-EpnS","executionInfo":{"status":"ok","timestamp":1739823597763,"user_tz":360,"elapsed":9,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# img, label = next(iter(source_loader))\n","\n","# # Batch size will now be 1, try changing the batch_size parameter above and see what happens\n","# print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n","# print(f\"Label shape: {label}\")\n","# print(len(label))"],"metadata":{"id":"tsh2XFhMFaY2","executionInfo":{"status":"ok","timestamp":1739823597764,"user_tz":360,"elapsed":9,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Models"],"metadata":{"id":"aIZ-DamVBbMH"}},{"cell_type":"code","source":["class FeatureExtractor(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    # Load pre-trained model\n","    self.resnet = models.resnet18(pretrained=True)\n","    # Remove last layer\n","    self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n","\n","\n","  def forward(self, x):\n","    x = self.resnet(x)\n","    x = x.view(x.size(0), -1)\n","    return x\n","\n","\n","class Classifier(nn.Module):\n","  def __init__(self, input_put, num_classes):\n","    super().__init__()\n","    self.classifier = nn.Linear(input_put, num_classes)\n","\n","  def forward(self, x):\n","    return self.classifier(x)\n"],"metadata":{"id":"_h8PZaY2Bdjw","executionInfo":{"status":"ok","timestamp":1739823597764,"user_tz":360,"elapsed":9,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Traing loop with MMD Loss"],"metadata":{"id":"jOEqiSvMGK_M"}},{"cell_type":"code","source":["feature_extractor = FeatureExtractor().to(device)\n","classifier = Classifier(512, 20).to(device)\n","\n","mmd_loss_fn = MMDLoss(kernel_type='rbf')\n","loss_fn = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(list(feature_extractor.parameters()) + list(classifier.parameters()), lr=1e-4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24gk0iB8L2jy","executionInfo":{"status":"ok","timestamp":1739823597965,"user_tz":360,"elapsed":209,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"0586c6f4-1ceb-4d9d-9e29-4932ab2604b6"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","  feature_extractor.train()\n","  classifier.train()\n","  total_loss = 0.0\n","  total_mmd_loss = 0.0\n","  total_cls_loss = 0.0\n","\n","  source_iter = iter(source_loader)\n","  target_iter = iter(target_train_dataloader)\n","\n","  num_batchs = min(len(source_loader), len(target_train_dataloader))\n","\n","\n","  for i in range(num_batchs):\n","    try:\n","        source_data, source_labels = next(source_iter)\n","    except StopIteration:\n","        source_iter = iter(source_loader)\n","        source_data, source_labels = next(source_iter)\n","\n","    try:\n","        target_data, _ = next(target_iter)\n","    except StopIteration:\n","        target_iter = iter(target_train_dataloader)\n","        target_data, _ = next(target_iter)\n","\n","    source_data = source_data.to(device)\n","    source_labels = source_labels.to(device)\n","    target_data = target_data.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    # Forward pass through the feature extractor\n","    source_features = feature_extractor(source_data)\n","    target_features = feature_extractor(target_data)\n","\n","    #Forward pass through the classifier\n","    source_logits = classifier(source_features)\n","\n","\n","    # Compute classification loss on source data\n","    cls_loss = loss_fn(source_logits, source_labels)\n","\n","    # Compute the MMD loss between the source and target features\n","    mmd_loss = mmd_loss_fn(source_features, target_features)\n","\n","    # Total loss\n","    loss = cls_loss + mmd_loss\n","\n","    # backward pass and optimizer\n","    loss.backward()\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","    total_cls_loss += cls_loss.item()\n","    total_mmd_loss += mmd_loss.item()\n","\n","print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Classification Loss: {total_cls_loss:.4f}, MMD Loss: {total_mmd_loss:.4f}')"],"metadata":{"id":"1csF167wMxRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test Loop"],"metadata":{"id":"lD5D2c4Gm4BC"}},{"cell_type":"code","source":["def test_loop(feature_extractor, classifier, test_loader):\n","  feature_extractor.eval()\n","  classifier.eval()\n","\n","  total_correct = 0\n","  total_samples = 0\n","\n","  with torch.no_grad():\n","    for data, labels in test_loader:\n","      data = data.to(device)\n","      labels = labels.to(device)\n","\n","      # Extract features\n","      features = feature_extractor(data)\n","\n","      # classifier model preds\n","      preds = classifier(features)\n","\n","      _, predicted = torch.max(preds.data, 1)\n","      total_samples += labels.size(0)\n","      correct += (predicted == labels).sum()\n","\n","\n","  accuracy = 100 * correct.item() / total_samples\n","  print(f'Accuracy of the model on the target test images: {accuracy:.2f}%')\n","\n","\n","# Evaluate the model\n","test_loop(feature_extractor, classifier, target_test_dataloader)"],"metadata":{"id":"7fqIQHwfm8Vc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Results and Observations**\n","\n","Challenges and Solutions:\n","\n","*   Long training time\n","    - Use a stronger GPU\n","*   Low Acuuracy due to only 4 common classes between the two Domains\n","    - Change the source dataset or use differnt domains to practice MMD\n","\n"],"metadata":{"id":"kN3aESVUh44f"}}]}